\section{Conclusion and Discussion}
Problems with compatibility with different versions of ubuntu and ROS.  

\subsubsection{Discussions}
This system is based on a background segmentation to find moving objects in the scene which is used to extract points in the depth image. This is only one way of solving this, another more simple way of doing it would be to check at the depth image at once and extract information from it. For instance if the collaboration area would only consist of the floor and a robot objects would be found by thresholding the depth value in the image, that is removing what is floor in the image. This solution one might find more intuitive and simple but it does not take the movement of objects in the scene into account. Another disadvantage when having a background segmentation determining what to use in the image is that there are sometimes parts of objects that are misclassified as background. This problem makes the system more inaccurate and can make a distance between object and robot vary. In an area where there are objects, such as walls, visible for the camera one might prefer using a background segmentation since objects that are not moving will be removed and therefore the calculations needed will reduce significantly.  

One option in the system is to turn off tracking. If it is guaranteed that no one will approach the robot and then stand still for a long time an alternative can be to use the background modelling without the tracking.

In this project it has only been one kinect camera in use, this means that the position of the camera is of high importance since it needs to cover up the entire collaboration area. Unfortunately this was not possible to achieve and the camera setup for this project was therefore not perfect. The resulting camera position was not exactly over the robot, meaning that the system received a tilted view of the scene. A tilted view of the scene can result in objects which in fact are not connected to each other to be interpreted as connected, that is one object instead of two. In a more ideal case, it would be preferred to use more than one camera or have the camera placed exactly above the robot. 

\subsubsection{Uncertainty of calibration}
There is an uncertainty in the accuracy of the calibration. While the computed relation between the known calibration pattern and the camera is based on very robust methods the static calibration between the robot and the pattern is manually measured. Even minor angular errors can give large discrepancy between the robots frame of reference and the cameras frame of reference. To mitigate this the calibration pattern was placed as flat as possible relative to the ground, since this angle is easier to verify than other angles (using e.g. a spirit level). This angle is not optimal from the perspective of detecting the pattern and therefore a rather large calibration pattern is used to ensure detection.

In a more optimal situation the pattern would be painted on something which is mechanically forced into a well known high precision static transform to the robot.

An example of such a system would be to use painted QR codes on different known spots with a large relative distance between them. Since QR-codes are designed to be easily detected with cameras and can convey an orientation as well as an identifier then the system could recalibrate as long as it can see at least one of them (though for the sake of robustness it probably shouldnâ€™t unless it can see four of them). The feasibility of such a system is however out of the scope of this project.
